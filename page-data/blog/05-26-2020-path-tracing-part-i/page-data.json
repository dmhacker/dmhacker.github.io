{"componentChunkName":"component---src-templates-blog-post-js","path":"/blog/05-26-2020-path-tracing-part-i/","result":{"data":{"markdownRemark":{"id":"bb3af956-be73-516c-b512-71769d89caff","excerpt":"Ray tracing and to a greater extent, its sister algorithm, path tracing, have always \nfascinated me, beginning back in my high school days. At its core, the…","html":"<p>Ray tracing and to a greater extent, its sister algorithm, path tracing, have always\nfascinated me, beginning back in my high school days. At its core, the idea of ray\ntracing is remarkably elegant: simulate the bounces of light in a scene and capture\nthose photons that manage to make it to a camera. This is unlike rasterization,\nwhich, although fast, is inelegant and not physically accurate.</p>\n<p>In fact, here was one of the very first images I rendered back then with a\nsimple recursive ray tracer:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/a68668989b651fd0d2325eec2922a43b/dc93e/high_school.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 53.01455301455301%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAQFAQL/xAAUAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAGd0xozx8T/xAAXEAEBAQEAAAAAAAAAAAAAAAACEwEg/9oACAEBAAEFAp5Mka48f//EABURAQEAAAAAAAAAAAAAAAAAABBB/9oACAEDAQE/AYf/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAcEAACAAcAAAAAAAAAAAAAAAABEQAgITEyQVH/2gAIAQEABj8CLFexhqzl/8QAHBAAAgICAwAAAAAAAAAAAAAAABEBIRBRMXGR/9oACAEBAAE/IYjvWpuC6NHs2dhCjWP/2gAMAwEAAgADAAAAEL8P/8QAFhEBAQEAAAAAAAAAAAAAAAAAAREQ/9oACAEDAQE/EBKpn//EABURAQEAAAAAAAAAAAAAAAAAABBB/9oACAECAQE/EKf/xAAaEAEAAwEBAQAAAAAAAAAAAAABABEhUTFx/9oACAEBAAE/ED9FDipyoapwIRcHXqAcQSIGA+Eo5P/Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Old Ray Tracer\"\n        title=\"Old Ray Tracer\"\n        src=\"/static/a68668989b651fd0d2325eec2922a43b/c739e/high_school.jpg\"\n        srcset=\"/static/a68668989b651fd0d2325eec2922a43b/8ee9c/high_school.jpg 148w,\n/static/a68668989b651fd0d2325eec2922a43b/ebbe7/high_school.jpg 295w,\n/static/a68668989b651fd0d2325eec2922a43b/c739e/high_school.jpg 590w,\n/static/a68668989b651fd0d2325eec2922a43b/5413e/high_school.jpg 885w,\n/static/a68668989b651fd0d2325eec2922a43b/4efde/high_school.jpg 1180w,\n/static/a68668989b651fd0d2325eec2922a43b/dc93e/high_school.jpg 1924w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>I am very fortunate to be a student in Professor <a href=\"http://cseweb.ucsd.edu/~ravir/\">Ravi Ramamoorthi</a>’s\n<a href=\"http://cseweb.ucsd.edu/~viscomp/classes/cse168/sp20/168.html\">CSE 168</a> class at UCSD, where we get to learn about physically-based\nrendering methods as well as other modern path tracing algorithms. Over\nthe duration of this course, we’ve begin incrementally building up a modern\npath tracer, starting with a path tracer handling only direct light and\nadding indirect lighting, next event estimation, Russian Roulette,\nphysically-based GGX BRDFs, and multiple importance sampling features.\nThe path tracer also runs using Nvidia’s OptiX 6.5 framework,\nwhich allowed me to take advantage of my laptop’s GTX 1050 to gain\na significant speed improvement.</p>\n<p>For my final project in this class, I will be extending the path\ntracer with the following algorithms:</p>\n<ul>\n<li>Depth of Field Effects</li>\n<li>Texture Mapping</li>\n<li>Glass Microfacet BTDFs</li>\n<li>Smoke via Volumetric Path Tracing</li>\n</ul>\n<p>Additionally, I have crafted my own scene to best showcase these\nnew features. Please see below for a description of this process.</p>\n<p>Do note that this is still a work in progress (hence the part I).\nThe glass BTDF and volumetric path tracing are currently incomplete.</p>\n<h4>Creating a Reference Image</h4>\n<p>My first step was to create a reference image. I wanted to use objects\nin my scene that I actually have interacted with on a daily basis\nto make it more personal. It was also convenient for me to use these\nobjects since they were readily available in my room.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto;  max-width: 590px;\"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/ddcdf63f35bb662298d9ca167e5fdc5c/243ce/reference.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 75%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAMEBf/EABUBAQEAAAAAAAAAAAAAAAAAAAID/9oADAMBAAIQAxAAAAGV63RtGaAz/8QAGRAAAwEBAQAAAAAAAAAAAAAAAQIDABES/9oACAEBAAEFApLiKsrSo5PWxm3Pe//EABcRAAMBAAAAAAAAAAAAAAAAAAABAhL/2gAIAQMBAT8BcmUf/8QAGBEAAgMAAAAAAAAAAAAAAAAAAAECERL/2gAIAQIBAT8BUqNH/8QAGxAAAgIDAQAAAAAAAAAAAAAAABEBIQISImH/2gAIAQEABj8CHOVejo5lD3LP/8QAHBAAAgMAAwEAAAAAAAAAAAAAASEAETFBcaHh/9oACAEBAAE/IQDNy+5fIHRWp85y2BQHsNCxcZFQOf/aAAwDAQACAAMAAAAQl8//xAAXEQADAQAAAAAAAAAAAAAAAAAAARFh/9oACAEDAQE/ELqJGh//xAAXEQADAQAAAAAAAAAAAAAAAAAAAREx/9oACAECAQE/EKq3pR//xAAcEAEBAAMBAAMAAAAAAAAAAAABEQAhMUGBkbH/2gAIAQEAAT8QtisQCHj8xiynUHomFdggd0+DEXOqlrAkRFBL9ZINBph3P//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Reference\"\n        title=\"Reference\"\n        src=\"/static/ddcdf63f35bb662298d9ca167e5fdc5c/c739e/reference.jpg\"\n        srcset=\"/static/ddcdf63f35bb662298d9ca167e5fdc5c/8ee9c/reference.jpg 148w,\n/static/ddcdf63f35bb662298d9ca167e5fdc5c/ebbe7/reference.jpg 295w,\n/static/ddcdf63f35bb662298d9ca167e5fdc5c/c739e/reference.jpg 590w,\n/static/ddcdf63f35bb662298d9ca167e5fdc5c/5413e/reference.jpg 885w,\n/static/ddcdf63f35bb662298d9ca167e5fdc5c/4efde/reference.jpg 1180w,\n/static/ddcdf63f35bb662298d9ca167e5fdc5c/243ce/reference.jpg 4032w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        loading=\"lazy\"\n      />\n  </a>\n    </span></p>\n<p>I have specific reasons for using and positioning these objects\nlike so. You can read more in my <a href=\"http://dmhacker.github.io/cse168/proposal.pdf\">proposal</a>.</p>\n<h4>Crafting a 3D Scene using Blender</h4>\n<p>The next step was to recreate this scene using a 3D computer\ngraphics modeling suite. I choose to use Blender, since it\nworks reasonably well on Linux, is free, and comes with\nmany handy features, such as imports/exports to and from\na variety of different formats, path tracing and rasterizer\npreviews, and the ability to attach realistic materials to\nobjects in the scene (including specifying their BRDFs).</p>\n<p>Before this experience, I had never actually worked with\nany 3D rendering software, much less an advanced tool like\nBlender, so it took me a while before I felt comfortable\nusing it. Blender’s learning curve reminds me of that of\nVim: it’s difficult to navigate around at first when you\ndon’t know any of the keybindings and tools, but as you\nlearn more and more, you eventually reach a critical point\nat which it suddenly seems much easier.</p>\n<p>In addition to learning Blender, I also spent several\nhours actually making my scene. There were two challenging\nparts to this:</p>\n<ol>\n<li>Finding the models and textures to use. I scoured\na bunch of free sites, such as Sketchpad and Clara,\nsearching for the right, free models. Ideally, I would\nsearch for .obj/.mtl or .fbx file formats, since they\ncome with good textures and Blender seems to like them.\nI should note that I was even unable to find a suitable\nmodel for the Raspberry Pi in my original image, so I\nswapped it out with a microcontroller breadboard.</li>\n<li>Scaling and positioning all of the models. Importing\nthem is half the battle. In addition to all of the models,\nI also set up where my camera and light should be.</li>\n</ol>\n<p><a href=\"./blender.png\">Blender WIP</a></p>\n<p>At the end, I found myself with a pretty nice scene. I\nrendered a preview using Blender’s Cycles engine, which\nprovided me with a benchmark from which I could gauge\nprogress on my own path tracer.</p>\n<p><a href=\"./preview.png\">Cycles Preview</a></p>\n<p>Do note that I didn’t enable any smoke or depth effects\nwhen using Cycles, so this image isn’t completely accurate.\nI also swapped out the wood textures and made the tungsten\ncube less reflective, so this image is slightly outdated.\nIt’s still a good preview nonetheless.</p>\n<h4>Importing the Scene into my Path Tracer</h4>\n<p>At this point, my scene was basically complete. However,\nI needed some way to transfer the scene out of Blender\nand into my own path tracer, since my original path tracer\nwas only configured to read specially formatted CSE 168\n.test files.</p>\n<p>There were several options to choose from:</p>\n<ul>\n<li>.dae “Collada” format: This format was appealing for\nmany reasons. It’s human readable, it includes camera\nand lighting data, it has support for “extra” data that\ncan be appended to each object, and it’s Blender’s first\nchoice for exports. However, I ended up not using it because\nBlender kept distorting the rubber handle on my screwdriver,\nso when I re-imported it, the mesh looked completely wrong.</li>\n<li>.fbx format: This format is probably the smallest out of\nmy options because it’s not human readable. It’s also proprietary.\nThose two issues dissuaded me from using it.</li>\n<li><strong>.obj/.mtl “Wavefront Object/Material” format</strong>: This is\nthe choice I ended up settling on. </li>\n</ul>\n<p>The Wavefront object file format is extremely well-known, and\nthere exist many C++ libraries such as <a href=\"https://github.com/Bly7/OBJ-Loader\">BLY-7’s OBJ-Loader</a>\nthat can handle it very well. The object file format has\nsupport for texture and shading data. Texture coordinates\nare kept inside vertices, and links to the texture files\nare kept inside the material file. </p>\n<p>There were two main problems with the Wavefront object file\nformat unfortunately. I had to work around these:</p>\n<ul>\n<li>No camera or lighting data saved. Object file formats describe\nobjects, not scenes. My workaround was to store this data in a\nseparate .test CSE 168 file. This scene file then referenced my\nobject file to load the objects in the scene. The camera,\nlighting, and other parameters were kept inside the overarching\nscene file. I manually copied all of the positions of the camera and\nthe light from my scene; one annoyance with this approach was that\nBlender keeps its positions in Y-up format (the Y-axis is the up\ndirection) where my path tracer was using Z-up format. In the object\nfile, Blender will automatically convert its Y-up system to a\nZ-up system, so thankfully, that was all taken care of.\nBut, unfortunately, that conversion had to take place manually for\nmy camera and light; it took me a bit to figure all of that out.</li>\n<li>Object files only support primitive Blinn-Phong shading parameters,\nwhich are non-PB. However, my scene in Blender was using physically-based\n“principled” microfacet BRDFs in all internal material data. Blender’s\nsolution to this is to encode all of this PBR information into the Blinn-Phong\nparameters. For example, when Blender exports its BRDF “roughness” parameter,\nit will turn this into a shininess value in the range of [0, 900] by\nsubtracting the roughness value from 1, multiplying it by 30, and the squaring\nit. My solution to this problem was just to adapt Blender’s import source code\nappropriately in my own path tracer.</li>\n</ul>\n<h5>Rendering the Outline</h5>\n<p>To make sure everything was working, I first disabled all of my shading and\nrecursive path tracing. Instead, whenever I hit an object, I would just\nreturn the object’s diffuse color immediately. This allowed me to continually\ntweak my camera’s parameters until I felt satisfied that I had replicated\nthe original scene in Blender to a reasonable extent.</p>\n<p>This left me with the following scene.</p>\n<p><a href=\"./render0.png\">No Shading Render</a></p>\n<p>It’s not very pretty, but you can see the outlines of all of the different\nobjects, which is exactly what I needed.</p>\n<h5>Applying Shading</h5>\n<p>The next step is to add back shading. This served two purposes.</p>\n<ol>\n<li>I could gauge the position of my light in the scene using the shading\nof the objects as reference.</li>\n<li>I could better tweak my interpretation specular, diffuse, and roughness\nparameters until I was satisfied that they looked natural.</li>\n</ol>\n<p>After some fiddling, I managed to get this scene.</p>\n<p><a href=\"./render1.png\">With Shading Render</a></p>\n<p>Perfect. The materials with no textures, such as the tungsten cube and\nRubik’s cube, looked very realistic without much adjustment, owing to\nthe microfacet BRDF implemented previously. For more context on what\nthis BRDF is, please see <a href=\"http://www.graphics.cornell.edu/~bjw/microfacetbsdf.pdf\">this paper</a>.\nThe microfacet distribution used is GGX.</p>\n<h4>The Depth of Field Effect</h4>\n<p>TODO</p>\n<h4>Texture Mapping</h4>\n<p>TODO</p>\n<h4>What’s Next?</h4>\n<p>TODO</p>","frontmatter":{"title":"Realistic Path Tracing (part I)","date":"May 27, 2020","description":"Crafting a scene, implementing textures and depth-of-field effects using OptiX"}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/blog/05-26-2020-path-tracing-part-i/","previous":{"fields":{"slug":"/blog/12-04-2019-starting-a-blog/"},"frontmatter":{"title":"Starting My Own Blog"}},"next":null}}}