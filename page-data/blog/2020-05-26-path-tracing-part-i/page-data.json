{"componentChunkName":"component---src-templates-blog-post-js","path":"/blog/2020-05-26-path-tracing-part-i/","result":{"data":{"markdownRemark":{"id":"4f9cf248-2d9b-57e6-8726-a5185c055c64","excerpt":"Ray tracing and to a greater extent, its sister algorithm, path tracing, have always\nfascinated me. At its core, the idea of ray tracing is remarkably elegant…","html":"<p>Ray tracing and to a greater extent, its sister algorithm, path tracing, have always\nfascinated me. At its core, the idea of ray tracing is remarkably elegant:\nsimulate the bounces of light in a scene and capture\nthose photons that manage to make it to a camera. This is unlike rasterization,\nwhich, although fast, is inelegant and not physically accurate.</p>\n<p>In fact, here was one of the very first images I rendered back in high school\nwith a simple recursive ray tracer written in Java:</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/a68668989b651fd0d2325eec2922a43b/1047b/high_school.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 52.70270270270271%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAQFAQL/xAAUAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAGd0xozx8T/xAAXEAEBAQEAAAAAAAAAAAAAAAACEwEg/9oACAEBAAEFAp5Mkapcf//EABURAQEAAAAAAAAAAAAAAAAAABBB/9oACAEDAQE/AYf/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAbEAABBAMAAAAAAAAAAAAAAAABABEgUSEyQf/aAAgBAQAGPwIuM2tOXH//xAAcEAABBAMBAAAAAAAAAAAAAAARAAEhURAxQXH/2gAIAQEAAT8hZudKC0p+Tc2te0EGrH//2gAMAwEAAgADAAAAEL8P/8QAFhEBAQEAAAAAAAAAAAAAAAAAAREQ/9oACAEDAQE/EBKpn//EABURAQEAAAAAAAAAAAAAAAAAABBB/9oACAECAQE/EKf/xAAZEAEAAwEBAAAAAAAAAAAAAAABABFRIXH/2gAIAQEAAT8QP0pPFTKmRTVEL6gHiCMQOA8JRk//2Q=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Old Ray Tracer\"\n        title=\"Old Ray Tracer\"\n        src=\"/static/a68668989b651fd0d2325eec2922a43b/1c72d/high_school.jpg\"\n        srcset=\"/static/a68668989b651fd0d2325eec2922a43b/a80bd/high_school.jpg 148w,\n/static/a68668989b651fd0d2325eec2922a43b/1c91a/high_school.jpg 295w,\n/static/a68668989b651fd0d2325eec2922a43b/1c72d/high_school.jpg 590w,\n/static/a68668989b651fd0d2325eec2922a43b/a8a14/high_school.jpg 885w,\n/static/a68668989b651fd0d2325eec2922a43b/fbd2c/high_school.jpg 1180w,\n/static/a68668989b651fd0d2325eec2922a43b/1047b/high_school.jpg 1924w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>I am very fortunate to be a student in Professor <a href=\"http://cseweb.ucsd.edu/~ravir/\">Ravi Ramamoorthi</a>’s\n<a href=\"http://cseweb.ucsd.edu/~viscomp/classes/cse168/sp20/168.html\">CSE 168</a> class at UCSD, where we get to learn about physically-based\nrendering methods as well as other modern path tracing algorithms. Over\nthe duration of this course, we’ve begin incrementally building up a modern\npath tracer, starting with a path tracer handling only direct light and\nadding indirect lighting, next event estimation, Russian Roulette,\nphysically-based GGX BRDFs, and multiple importance sampling features.\nThe path tracer also runs using <a href=\"https://developer.nvidia.com/optix\">NVIDIA’s OptiX 6.5</a> framework,\nwhich allowed me to take advantage of my laptop’s GTX 1050 to gain\na significant speed improvement.</p>\n<p>For my final project in this class, I will be extending the path\ntracer with the following algorithms:</p>\n<ul>\n<li>Depth of Field Effects</li>\n<li>Texture Mapping</li>\n<li>Glass Microfacet BTDFs</li>\n<li>Smoke via Volumetric Path Tracing</li>\n</ul>\n<p>Additionally, I have crafted my own scene to best showcase these\nnew features. Please see below for a description of this process.</p>\n<p>Do note that this is still a work in progress (hence the part I).\nThe glass BTDF and volumetric path tracing are currently incomplete.</p>\n<h4>Creating a Reference Image</h4>\n<p>My first step was to create a reference image. I wanted to use objects\nin my scene that I actually have interacted with on a daily basis\nto make it more personal. It was also convenient for me to use these\nobjects since they were readily available in my room.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/ddcdf63f35bb662298d9ca167e5fdc5c/d2602/reference.jpg\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 75%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAwQA/8QAFQEBAQAAAAAAAAAAAAAAAAAAAwL/2gAMAwEAAhADEAAAAZmNgaTX5J//xAAZEAADAQEBAAAAAAAAAAAAAAABAgMAERL/2gAIAQEAAQUCkuK1KtKjk9bGbb3v/8QAFxEAAwEAAAAAAAAAAAAAAAAAAAECEv/aAAgBAwEBPwFyZR//xAAXEQADAQAAAAAAAAAAAAAAAAAAAREC/9oACAECAQE/AVqFP//EABkQAAMBAQEAAAAAAAAAAAAAAAARIRIBUf/aAAgBAQAGPwIep4OE6h7Kf//EAB0QAAIDAAIDAAAAAAAAAAAAAAERACExEEFxobH/2gAIAQEAAT8hAM3H5jFIXsXx3bAUHuGAMrrPkrQXP//aAAwDAQACAAMAAAAQDA//xAAYEQACAwAAAAAAAAAAAAAAAAAAAREhYf/aAAgBAwEBPxCekjQ//8QAFxEAAwEAAAAAAAAAAAAAAAAAAAERMf/aAAgBAgEBPxCqt6Wf/8QAHBABAQACAwEBAAAAAAAAAAAAAREAITFBYYGx/9oACAEBAAE/EAEpiAQ6fmN34qqHZM3gqHcnwM5fddXHL7FgD5jyDWjnP//Z'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Reference\"\n        title=\"Reference\"\n        src=\"/static/ddcdf63f35bb662298d9ca167e5fdc5c/1c72d/reference.jpg\"\n        srcset=\"/static/ddcdf63f35bb662298d9ca167e5fdc5c/a80bd/reference.jpg 148w,\n/static/ddcdf63f35bb662298d9ca167e5fdc5c/1c91a/reference.jpg 295w,\n/static/ddcdf63f35bb662298d9ca167e5fdc5c/1c72d/reference.jpg 590w,\n/static/ddcdf63f35bb662298d9ca167e5fdc5c/a8a14/reference.jpg 885w,\n/static/ddcdf63f35bb662298d9ca167e5fdc5c/fbd2c/reference.jpg 1180w,\n/static/ddcdf63f35bb662298d9ca167e5fdc5c/d2602/reference.jpg 4032w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>I have specific reasons for using and positioning these objects\nlike so. You can read more in my <a href=\"http://dmhacker.github.io/cse168/proposal.pdf\">proposal</a>.</p>\n<h4>Crafting a 3D Scene using Blender</h4>\n<p>The next step was to recreate this scene using a 3D computer\ngraphics modeling suite. I choose to use <a href=\"https://www.blender.org/\">Blender</a>, since it\nworks reasonably well on Linux, is free, and comes with\nmany handy features, such as imports/exports to and from\na variety of different formats, path tracing and rasterizer\npreviews, and the ability to attach realistic materials to\nobjects in the scene (including specifying their BRDFs).</p>\n<p>Before this experience, I had never actually worked with\nany 3D rendering software, much less an advanced tool like\nBlender, so it took me a while before I felt comfortable\nusing it. Blender’s learning curve reminds me of that of\nVim: it’s difficult to navigate around at first when you\ndon’t know any of the keybindings and tools, but as you\nlearn more and more, you eventually reach a critical point\nat which it suddenly seems much easier.</p>\n<p>In addition to learning Blender, I also spent several\nhours actually making my scene. There were two challenging\nparts to this:</p>\n<ol>\n<li>Finding the models and textures to use. I scoured</li>\n</ol>\n<p>a bunch of free sites, such as Sketchpad and Clara,\nsearching for the right, free models. Ideally, I would\nsearch for .obj/.mtl or .fbx file formats, since they\ncome with good textures and Blender seems to like them.\nI should note that I was even unable to find a suitable\nmodel for the Raspberry Pi in my original image, so I\nswapped it out with a microcontroller breadboard.</p>\n<ol start=\"2\">\n<li>Scaling and positioning all of the models. Importing</li>\n</ol>\n<p>them is half the battle. In addition to all of the models,\nI also set up where my camera and light should be.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/35cf043e911e64ae60bede461818a7b1/e8631/blender.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 64.1891891891892%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAIAAAAmMtkJAAAACXBIWXMAAA7EAAAOxAGVKw4bAAACtklEQVQoz03N20/TYAAF8CYS33wzaCT0flnbrV0vW2m3ruvWtV/bFbZ2YxJA7kgETVRiTEyMUWBMQJCh6BIwGPXFf9IMX/jlvJ5zoJJpavqEoqpVx2nErWYzBn7QiFudmdl2ZyZuteOk1UwS1697fhhGDats25YpihlFzkILiwv1mmsZhd2Px98v/5wNrk6+XpwNfvYHVyfnlyfnl5++DA4+f+ke9Xc/Hh+cnG482SpXgFjwZN2BLn//Wp9M7KxyeDb48evv18GP/reL4/754eGn3n5v7/27/b3u0efT/YOD3tHx0Wl/bW2tVCgohinldWjKtC2O0LP8q2drG/X8clXdbAZtYWx7fnLn9YtJZmyhWurtvN/f2+l2D7t7u0tLi3pOlnOqIIlQ2SgWVUWTxJXYfTxlJzI1ZxWrd0dezjZ7b99Ml/PLsdO29JUkWYo7y3W/WbOZVIplWZbjIKNYlFVVluSVpLW9/jTOMP7922B0ZFpAV21901QWQsPh8CkplahEohCems1KsiiKmUwG0jRNkqScmpup1uaq/hRLgtFb4N5IiNx5mBMe5XJtmU8UtsE9WG0WPmzNP19fsizLdd1isQiZpqldK0iCIfA6R2kUbLCEnsI1htDTKS3Nl2XB4DBDYgNrwqtYHgD1et1xHMjzvMo1kqZhFENxgqJpgqJhnIBxDEWxcQRBcQLBCQTFxsZhmqKCIIyiaFj2fT+KIs/zSJKEYZihSKek8ymG5Tl5QhU4DkNRFEWugyIITNEMAKBWq9m2DUVR1Ol0wjAkCAKGh8OOXaIpMp/XwiDACRy9AUEQikn5QeD7/rAMAPB933Xd/88YhqXTGXh8XFbytgNgGEZuGK6TpFurAQAqlQrkOI5t26ZpptNplmVpiiIJgmEYYUikKIphGPYGnudN0yyXy6VS6R8mU88AQus2QwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Blender Application\"\n        title=\"Blender Application\"\n        src=\"/static/35cf043e911e64ae60bede461818a7b1/fcda8/blender.png\"\n        srcset=\"/static/35cf043e911e64ae60bede461818a7b1/12f09/blender.png 148w,\n/static/35cf043e911e64ae60bede461818a7b1/e4a3f/blender.png 295w,\n/static/35cf043e911e64ae60bede461818a7b1/fcda8/blender.png 590w,\n/static/35cf043e911e64ae60bede461818a7b1/efc66/blender.png 885w,\n/static/35cf043e911e64ae60bede461818a7b1/c83ae/blender.png 1180w,\n/static/35cf043e911e64ae60bede461818a7b1/e8631/blender.png 2949w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>At the end, I found myself with a pretty nice scene. I\nrendered a preview using Blender’s Cycles engine, which\nprovided me with a benchmark from which I could gauge\nprogress on my own path tracer.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/f344b833680d6c590967747315266aaa/29114/preview.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.08108108108109%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAAAsTAAALEwEAmpwYAAAC7UlEQVQozx3MW29aBQDA8aOLzo5by8o13M4BDgc4FA5QGHNcWrdSwBZI2/W2bNhSu6oNV3uBmvmiD82cMT74Bfa+ty3ON1/cjIm66INZsq/yN+vD7/UnFAp55ksl1HCQ9vCA0yc/8u+b/3h0fk6tVqNUKFIqFmnU6xTzefK5HAu3blKpVi6U5koU8nmKhcIFQZuJkYzHSUfDHB/1+OXlr7x5/ZqvRiO0mEr94wrX0imq5TIpNUpqJk4+m7swl8tzbSaJpqok3z4zMYREWCEZCZMOBTnrdvjn71e8+utPTnpdAnYrd9bX2FpdoTZfYjYk82EixI3FRZKVBlpjhWTlOumITDqskI4oCEG/RMjvRxW9fNG6y4sXv/HH7y/p79wj7bCymJT56eE+O40yJclHc9ZFfT9I5mGE2Pd2Eo9szMd9zAX9lEISguJ1I3s9BF1O7taX+PnpU54/e0ZnZYlln4VmwEFn+zrf9tbZiAS4rTlY2pgiNdAROBKIDS+xGXexrXhYD3sQbooOypKTstfCmiryTe+Q78Yn7C/kqXiuUhHt1KJ+WpUcnVmZjZidytpVMn0TgbN3yQ4u09UcdGJOPos5EWo+K03Jzqo4zbZsZ3Rvi/NBh3HzFvsBI/dVBzsJH63QNAPNxbFmo1uaolM3cHj7HfrLlzlL2jjVbBwlbAhVj5maZ5Jll57NoIWz1hY/jI75erXGrmRmV7bRzYQZ5hJ01QidsMSuZmZYNPBg4RLj+QnGcQvj+DSncQvChsfKniLyparwYDbFydxH7GVyNAJBFtwe8hY7TSnIYTzNQSpLwyeTtU6xmTXQr71HrzhBP2JmEJ2iGzEjjFSN55V1Hher3HFJaBN6UpNmcj6RmM2GZDLhnppk+soEbqOOjFXHpqhjTzFwENXxuWLkvmy6cCCbEFo+kaE/wg2jHqf+Coosslyv8mm7jd/jxvjB+3gddtJWI2tuPW3RQFsy0BKNfPKW38RewEjbb2DHb+B/x/iFLi9BXcEAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Cycles Preview\"\n        title=\"Cycles Preview\"\n        src=\"/static/f344b833680d6c590967747315266aaa/fcda8/preview.png\"\n        srcset=\"/static/f344b833680d6c590967747315266aaa/12f09/preview.png 148w,\n/static/f344b833680d6c590967747315266aaa/e4a3f/preview.png 295w,\n/static/f344b833680d6c590967747315266aaa/fcda8/preview.png 590w,\n/static/f344b833680d6c590967747315266aaa/efc66/preview.png 885w,\n/static/f344b833680d6c590967747315266aaa/c83ae/preview.png 1180w,\n/static/f344b833680d6c590967747315266aaa/29114/preview.png 1920w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Do note that I didn’t enable any smoke or depth effects\nwhen using Cycles, so this image isn’t completely accurate.\nI also swapped out the wood textures and made the tungsten\ncube less reflective, so this image is slightly outdated.\nIt’s still a good preview nonetheless.</p>\n<h4>Importing the Scene into my Path Tracer</h4>\n<p>At this point, my scene was basically complete. However,\nI needed some way to transfer the scene out of Blender\nand into my own path tracer, since my original path tracer\nwas only configured to read specially formatted CSE 168\n.test files.</p>\n<p>There were several options to choose from:</p>\n<ul>\n<li>.dae “Collada” format: This format was appealing for</li>\n</ul>\n<p>many reasons. It’s human readable, it includes camera\nand lighting data, it has support for “extra” data that\ncan be appended to each object, and it’s Blender’s first\nchoice for exports. However, I ended up not using it because\nBlender kept distorting the rubber handle on my screwdriver,\nso when I re-imported it, the mesh looked completely wrong.</p>\n<ul>\n<li>.fbx format: This format is probably the smallest out of</li>\n</ul>\n<p>my options because it’s not human readable. It’s also proprietary.\nThose two issues dissuaded me from using it.</p>\n<ul>\n<li><strong>.obj/.mtl “Wavefront Object/Material” format</strong>: This is</li>\n</ul>\n<p>the choice I ended up settling on.</p>\n<p>The Wavefront object file format is extremely well-known, and\nthere exist many C++ libraries such as <a href=\"https://github.com/Bly7/OBJ-Loader\">BLY-7’s OBJ-Loader</a>\nthat can handle it very well. The object file format has\nsupport for texture and shading data. Texture coordinates\nare kept inside vertices, and links to the texture files\nare kept inside the material file.</p>\n<p>There were two main problems with the Wavefront object file\nformat unfortunately. I had to work around these:</p>\n<ul>\n<li>No camera or lighting data saved. Object file formats describe</li>\n</ul>\n<p>objects, not scenes. My workaround was to store this data in a\nseparate .test CSE 168 file. This scene file then referenced my\nobject file to load the objects in the scene. The camera,\nlighting, and other parameters were kept inside the overarching\nscene file. I manually copied all of the positions of the camera and\nthe light from my scene; one annoyance with this approach was that\nBlender keeps its positions in Y-up format (the Y-axis is the up\ndirection) where my path tracer was using Z-up format. In the object\nfile, Blender will automatically convert its Y-up system to a\nZ-up system, so thankfully, that was all taken care of.\nBut, unfortunately, that conversion had to take place manually for\nmy camera and light; it took me a bit to figure all of that out.</p>\n<ul>\n<li>Object files only support primitive Blinn-Phong shading parameters,</li>\n</ul>\n<p>which are non-PB. However, my scene in Blender was using physically-based\n“principled” microfacet BRDFs in all internal material data. Blender’s\nsolution to this is to encode all of this PBR information into the Blinn-Phong\nparameters. For example, when Blender exports its BRDF “roughness” parameter,\nit will turn this into a shininess value in the range of [0, 900] by\nsubtracting the roughness value from 1, multiplying it by 30, and the squaring\nit. My solution to this problem was just to adapt Blender’s import source code\nappropriately in my own path tracer.</p>\n<h5>Rendering the Outline</h5>\n<p>To make sure everything was working, I first disabled all of my shading and\nrecursive path tracing. Instead, whenever I hit an object, I would just\nreturn the object’s diffuse color immediately. This allowed me to continually\ntweak my camera’s parameters until I felt satisfied that I had replicated\nthe original scene in Blender to a reasonable extent.</p>\n<p>This left me with the following scene.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/5976a4b959bd32d00a3f52024ea8e182/d9199/render0.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.08108108108109%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACJElEQVQoz23QS08TURgG4K9iXOpP8A906cbEgCaKCzcsXBlLXECqJBIwnc7l9MJ0EmdEisDUnQujRSWioWAMNhhJI0GNEWsN0lAcpnPOmelcCtPI0ksq1JDI833bN3nzAgAEAgEAyGazvu/rut5oNEKhEPwVDAYlSUIIKYrS3t4BAIfa2uCfkZGRdDoty/LS0pLruoZheJ43NTXV29ujqhPxeJzjOJZlU6lUZ+c5ADhzuj2TURVFGR4eBr/Ftm1CCKUUY/xjZyejjk0/m5nJ5QYHBwRBSKXErq6LABAeEH7++l2v133fh2rLbpJSSgixbef1Qp7tuyDflDiO5xGf4IZ605dOPD7a8eb4vPG0prub1U2gByNWzR0XQ8zgNRRL8IiLscnr493nl4+d/XJ4nmTrtIGpcXCYYOzYzmxuLhKJIIQExMe4RL/cc/LVkVPv2xZxzqPbmGKghFJMKMb7nmBCvO2tQqHAMtEYap7AoeSt2MPK7Wl8d52uWrRGKQHTskzHNl1n7z2X2jVD23Q2tCeTk/1MhEeCIAjRaHQoKVq6s2U1TGLtFgRcXtc/ftYK7zZm8+X706vpe6XEnWJELvUlX4bZ0Yggi5IkSSlRnFBVTf9OTEzo3rRQeZT7dIVZ6WaKl5lSOPH1hrIWn6iMPdAm50j+LflQ3PhWXis3YYxN09w/DVRfLGpXZeP5Al5eIaWyqVVNp1ne9BzqOcS1rZb/d/0DeUDFAnRfomYAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"No Shading Render\"\n        title=\"No Shading Render\"\n        src=\"/static/5976a4b959bd32d00a3f52024ea8e182/fcda8/render0.png\"\n        srcset=\"/static/5976a4b959bd32d00a3f52024ea8e182/12f09/render0.png 148w,\n/static/5976a4b959bd32d00a3f52024ea8e182/e4a3f/render0.png 295w,\n/static/5976a4b959bd32d00a3f52024ea8e182/fcda8/render0.png 590w,\n/static/5976a4b959bd32d00a3f52024ea8e182/efc66/render0.png 885w,\n/static/5976a4b959bd32d00a3f52024ea8e182/d9199/render0.png 960w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>It’s not very pretty, but you can see the outlines of all of the different\nobjects, which is exactly what I needed.</p>\n<h5>Applying Shading</h5>\n<p>The next step is to add back shading. This served two purposes.</p>\n<ol>\n<li>I could gauge the position of my light in the scene using the shading</li>\n</ol>\n<p>of the objects as reference.</p>\n<ol start=\"2\">\n<li>I could better tweak my interpretation specular, diffuse, and roughness</li>\n</ol>\n<p>parameters until I was satisfied that they looked natural.</p>\n<p>After some fiddling, I managed to get this scene.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/a91f3a552580707557df865ebe34a3e5/d9199/render1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.08108108108109%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAClUlEQVQozz3Q3U9ScQDG8Z8OkOO1F14wwFgrwGTAERDOOdo55BzmjDEa2MtKba0ohA3BmNJxiqFI1BB8AZUXAaFghcrLOYA68aI117/UzK3P5Xd7bh4AAGhrawMAoAiy6vNlM+lUMmE2mW5iV1eXSqnEcdxgMIhEIgBAe3s7+I/NZkMQxGAwYBj2Li8fpJK7sZ3h4QcCgYDdweJxuf0wjKIoQRA6ne6Rflx4904Hi9XZ2QlBEODxeHw+n8PhqFQq8iOZTMSjO1sYhkn6eiUSqVgkhuUyDMNwfGh0zKg3WWTICI/fc6vnGoAgiM1mM5lMgUAwPz+fSMQ3Nzf0ej2KaMa1vQq5VCaToRhCDGqxp+o+kita4mhM8vsaHMVQoFQqhUJhd3c3l8udnZ29Hkc2bDNW64z9hUHVLxHC8n4UQ3BMO2iGe5dZfQGG2TX2ZtoyOT0JbDab0+mcm5tzOBxfgsFv+Vwud7i2uupyOSfMZolYrFQoEBTBBwntY+y2hylYAo51y9dAyOf7BEiSXPF6A+vrkXA4lUj+KBaPSqVcLpc5zLpdTvieZEClwjBsCBuamDJ5ylayYt8vxPL5fCqdAoHg51AkvBWN7ibi+WKhQlNUnf5ZKH4/yDjtdlgqvbmaIIi3FkvrqnVx1aJP6XK1fHxyDEL+wKbPH11Zi5Pegw+LhzPu7LQ99+xdUv/SrRnRKxDNwIBardbpdAuehcs/l+e/z5rnzUajQVEUWJuyhLTGsNYYJgw7o0/2jK+Sz99nrO7Cor8S2Ssl0om9/Vgsls1mG43Gxa/Waeu03qzTNF2jasD72kpqHm47Pfv+YHo7WszlT6qV42qlVD45qparNNVsNs/+qddpukFRdK1aq1UqlXK5/Bd5NwtxfrBdOQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"With Shading Render\"\n        title=\"With Shading Render\"\n        src=\"/static/a91f3a552580707557df865ebe34a3e5/fcda8/render1.png\"\n        srcset=\"/static/a91f3a552580707557df865ebe34a3e5/12f09/render1.png 148w,\n/static/a91f3a552580707557df865ebe34a3e5/e4a3f/render1.png 295w,\n/static/a91f3a552580707557df865ebe34a3e5/fcda8/render1.png 590w,\n/static/a91f3a552580707557df865ebe34a3e5/efc66/render1.png 885w,\n/static/a91f3a552580707557df865ebe34a3e5/d9199/render1.png 960w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Perfect. The materials with no textures, such as the tungsten cube and\nRubik’s cube, looked very realistic without much adjustment, owing to\nthe microfacet BRDF implemented previously. For more context on what\nthis BRDF is, please see <a href=\"http://www.graphics.cornell.edu/~bjw/microfacetbsdf.pdf\">this paper</a>.\nThe microfacet distribution used is GGX. BRDF importance sampling,\nnext event estimation, and multiple importance sampling were all\nenabled. 64 samples per pixel were taken. These contants remain\nthe same for all subsequent images.</p>\n<h4>The Depth of Field Effect</h4>\n<p>My next step was tackling depth of field. My current path tracer was\nassuming that a “pinhole” camera model is in use. This means that\nthe camera is treated as a fixed point in space; all camera rays\nstart at the camera’s origin position.</p>\n<p>Obviously, this is not physically accurate. In real life, cameras have a\nlens through which light is captured. Not all light is directed\nto a single point in the camera. This is what results in depth of field,\nwherein a camera is able to focus on an object and make the rest of the\nscene blurry. There are two parameters that control this depth of field\neffect:</p>\n<ol>\n<li>Focal distance. The focal distance of a camera specifies how far out</li>\n</ol>\n<p>it focuses. For a real camera, this is usually given in terms of millimeters.\n2. Aperture size. The aperture size controls the blurriness of the objects\nnot in focus. For a real camera, the aperture is usually specified in\nan inverted fashion, such that a small aperture usually means an out-of-focus\nbackground. A wide aperture, conversely, would indicate that the background\nis more in-focus.</p>\n<p>This effect can be simulated in my path tracer by making a modification\nto how my camera is set up. I added two additional parameters to describe\nmy camera, focal distance and aperture size. Then, I modified my camera ray\ngeneration algorithm as such:</p>\n<ol>\n<li>Calculate the camera’s origin and outgoing ray direction as one would</li>\n</ol>\n<p>in a pinhole camera.\n2. Generate a focal point by computing origin + direction * focal length.\n3. Generate any perpindicular vector to the outgoing ray direction\nand normalize it. I did this by taking the cross product of my camera’s\nup vector and the ray direction and then normalizing the result. This\nvector I call “offset vector 1”.\n4. Generate a third perpindicular vector by taking the cross product\nof offset vector 1 and the outgoing ray direction. I call this vector\n“offset vector 2”.\n5. Now, generate a random float between [-0.5, 0.5] and multiply it\nby the aperture size. Then, scale “offset vector 1” by this random\nfloat.\n6. Do the same for “offset vector 2”.\n7. Add both offset vector 1 and offset vector 2 to the camera origin.\nThis is the new origin of the outgoing ray.\n8. Finally, generate a new outgoing direction by subtracting the focal\npoint from the new origin and normalizing the subtraction.</p>\n<p>This algorithm effectively simulates an aperture. The ray’s origins are\nperturbed such that they could be anywhere in the aperture circle\nthat is perpindicular to the direction the camera is facing. If the aperture\nsize is set to a very large number, then the simulated aperture will be\nvery large and you will get rays that start at a completely different\ndistance from the origin of the camera. If the aperture is small, the\ncamera ray’s origins will be very close to the original camera position. If the\naperture is 0, then you effectively have a pinhole camera again.</p>\n<p>I set my camera to focus on the glass cup and use a big-enough aperture size\nto produce visibly distinct blurring.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/2d328193493ae4ea770ded38d206ed30/d9199/render2.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.08108108108109%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAClElEQVQozz2QW09SAQCATybIpUdf3JKABzY9KHLnCOd4AtwapoZEc5JrCa2lBVjbQbwELBFFBBuBGnEV5FKwvHE5x0M66aHl+k3NWn3v3/fwAcA/UBRd9/nyB7lMJmU2z1AoFAAAOjs7FXK5Wq02GAwgCN5oa7vZ3v5fAeh0OoPBoFKpcrl8dXU1u59JxGNjY6N9fXw6reMOiyWVSGAY1mg0IyM6g0HPB3totA4mk8lgMAAWi8Vhs293d0MQ5Ha5MunUp9geiqJi4YBgQAT29krEIgRB1Gp0ZPSRftIqVo2yOVwuh8PmcIBbTCadTqdQKDweb2V5OZ1K7u5EJiYmVMpB/XC/XCwUiUQwAmsQDTKtFPi4/A2Oakp6V6mGERiAIAgEwa6uLi6X63Bg6XR6Jxqx26xW27zFqJQJeiRiKYyo1LB2yCTpD9EEEcqUc/yFeW7GMgPY7XYMw5xOJ4Zh29uhUqlYLOT9/g3ngmPaZBKAfIVMdi0jGs0kwvNTecE2R9D2PhD2rfsAt9u95vVuBQLRD5FMJv21Ujk+OrwOFPJLCwuSPgGkUCAIgiKoyTLpwd+8w7FUJV4qlrK5fSAYCoajkd1YLJ5OlSrlGoHjZ8RhufIlm8Pm56VC4d/VWq12dm728ury8qpFNIlqvXpyegKEN7d21gOxNX/S5c06PXnbYsHyujj9MqN/uqi890CmUkKQUjmo0+lW3q58/9W6+HnevGiSJIkTOLBpngsPGyPDxqjm4cf7j5PGZ5knr/LWpbInUIsmjlIH6WQykYgXCgWSJFs/Wuetb2TzjCCIBt4AvM9tnqHxPYcrsRnK7X0qFz+f1msn9dpR9fS4Vq0TONkkz/9AnBE4ieNEo4E3arVatVr9DZDuDTPABe1OAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"DOF Render\"\n        title=\"DOF Render\"\n        src=\"/static/2d328193493ae4ea770ded38d206ed30/fcda8/render2.png\"\n        srcset=\"/static/2d328193493ae4ea770ded38d206ed30/12f09/render2.png 148w,\n/static/2d328193493ae4ea770ded38d206ed30/e4a3f/render2.png 295w,\n/static/2d328193493ae4ea770ded38d206ed30/fcda8/render2.png 590w,\n/static/2d328193493ae4ea770ded38d206ed30/efc66/render2.png 885w,\n/static/2d328193493ae4ea770ded38d206ed30/d9199/render2.png 960w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h4>Texture Mapping</h4>\n<p>Next up is texture mapping. When implementing texture mapping, I disabled\nmy depth of field effect temporarily so that I could verify that my\ntextures were appearing correctly on my background objects.</p>\n<p>Textures are very well understood in the realm of computer graphics,\nand there exist many different types of texture mappings, for example,\nroughness maps, normal maps, diffuse maps, and displacement maps. For\nthis project, I wanted to focus on diffuse maps, that is, images that\ntake the place of the diffuse color of the object.</p>\n<p>When Blender exports a scene as a Wavefront object file, it encodes\ntwo parameters into the resultant object and material files that are\nnecessary for texture mapping.</p>\n<ol>\n<li>In the object file, each vertex has a texture coordinate added to it.</li>\n</ol>\n<p>A texture coordinate (u, v) describes where that vertex maps to in the\ntexture image (more on this later). Note that vertices only have texture\ncoordinates if their corresponding mesh material has a mapped image.\n2. In the material file, the <code class=\"language-text\">map_Kd &lt;FILE></code> parameter specifies that a\nmaterial has a diffuse texture map (hence the <code class=\"language-text\">Kd</code>) located at the file\nfound at <code class=\"language-text\">&lt;FILE></code>.</p>\n<p>Because textures are so commonplace, NVIDIA’s OptiX framework piggy-backs\noff of CUDA’s own support for OpenGL textures and adds additional\nparameters specifying mipmap levels, read modes, and more. Most importantly,\nthey provide support for reading an image at a specific texture\ncoordinate, essentially abstracting away much of the math (behind the scenes,\nI set my texture sampler to linearly interpolate between pixels). This simplifies\nthe texture mapping process greatly, effectively reducing it to this process:</p>\n<ol>\n<li>Read the image into a GPU buffer that OptiX can pass to my CUDA shader.</li>\n<li>Attach this GPU buffer to an OptiX texture sampler object, which then yields</li>\n</ol>\n<p>an immutable device ID that can be used to query the sampler.\n3. Store this device ID in a separate GPU buffer. Store the device ID’s index\nin this buffer in each of the mesh’s triangles.\n4. Repeat steps 1-3 for all textures.\n5. Inside the shader, in the intersection program, calculate texture coordinates\nat a triangle hit point. Look up the device ID using the triangle’s device index\nand then use index into the associated texture sampler using the texture coordinate\nto produce a diffuse color.</p>\n<p>Steps 1-4 can be accomplished by reading the OptiX documentation (specifically,\nthe texture section 3.3). Step 5 is a bit more tricky.</p>\n<p>To get step 5 to work, it’s important to first understand briefly what barycentric\ncoordinates are. <a href=\"https://mathworld.wolfram.com/BarycentricCoordinates.html\">Barycentric coordinates</a>\nare the product of a special coordinate\nsystem that essentially assigns uniquely a triplet of numbers to every point in the\narea of the triangle. This triplet has the special property that the sum of its\ncomponents is 1. Whenever we intersect a ray with a triangle, we can calculate the\nbarycentric coordinates of the intersection point. But how does this relate to\ntexture mapping?</p>\n<p>The texture mapping process described in steps 1-4 assigns every\nvertex in every triangle a unique pair of coordinates, referred to as texture\ncoordinates. A texture coordinate uniquely maps to a point on the image, much\nlike a barycentric coordinate uniquely maps to a point on a triangle.</p>\n<p>When we calculate the barycentric coordinates of an intersection point, we also\nhave available the texture coordinates of each of the triangle’s 3 vertices. We\ncan then use the barycentric coordinates (u, v, 1 - u - v) to linearly interpolate\nbetween the 3 texture coordinates to produce a new texture coordinate. This new\ntexture coordinate can then be looked up in the image with this part being\nhandled by OptiX.</p>\n<p>Let’s try this out.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/4ffa12151f2406ce93a9c23235e3afa2/d9199/render3.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.08108108108109%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACeklEQVQozz3C2U/acAAA4F/LpVxSTqkcFnpQjgK1RdBaaruKiDJRnOBEcUN0E5AZzRzZYZxz6LYHF7P4tAf/0sWXffkAAABAEACACLj22jvnw0/D28vNtTUAwQAA25g1nYgLM7nlpSJFEgAAWKMB/+k0kF4LwwBg47Z2a/vq9vLm+kIWcgEfqoWAx4HEI2RmakrIZWVJLCjzmM+rAUCvhXUaCNiNGodZaxuFCI+592r74e7X3fBrkvRTGEpg/qAbiVMkz6ZnMpwsFxdX9qhYBjHCTpPGYdIALfwUAsBl1rVrlfsf32++DLIRlHAZirNYDJug8TCXYnIsl1li6I6f6HjJtIFE9LhbDybtOo9Fa9LDVj3UWFHvb75dD87UuFMM6ctZZyLkp8NhlolNp1hOwfF9iOzCOcEgBkeE8AiQSeNC5KlCGHZk5uJw58NuZS3tVKnRbAghJ9AYHkrFaZ5J8gI1uWsItnSLonEzYa0kLWA5bqwkTdW0uT5lbcv0ea14XiscSMFm1lSKO2ifL4ZjySjFxmNSllrdtlQall3J1sramtkxUE0bX7DGTW5ki7e01cj7rYVBXe2rkc6s5znjjfp9DIGxUZJLRAs8fbruOVl3dWX7kTj2Zg4BDc6+z6Pd6fBgJj1UFv6U63/rrcfq68dSc8AX5zGc8o/ToQBLhVYy+OkGelL19J/Z+xLSlRygw0QfhPpDfute2Pg9t3qXL/9USleS9FFIfc7HjgV/g0deZtzNnLuvoGflwLvl8WPF2RMdnbwL1EjnXpBqpQItDm3PYm8LTH91uldiDkT0MO/pzKNdyduXfWcq2leCPRXrKu6O5Dqacx2J7n9dUou5MPF+zAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Bad Texture Render\"\n        title=\"Bad Texture Render\"\n        src=\"/static/4ffa12151f2406ce93a9c23235e3afa2/fcda8/render3.png\"\n        srcset=\"/static/4ffa12151f2406ce93a9c23235e3afa2/12f09/render3.png 148w,\n/static/4ffa12151f2406ce93a9c23235e3afa2/e4a3f/render3.png 295w,\n/static/4ffa12151f2406ce93a9c23235e3afa2/fcda8/render3.png 590w,\n/static/4ffa12151f2406ce93a9c23235e3afa2/efc66/render3.png 885w,\n/static/4ffa12151f2406ce93a9c23235e3afa2/d9199/render3.png 960w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Wait … these textures look off! Even though the textures are associating\ncorrectly with each object, they aren’t being mapped properly, which is producing\nweird jagged lines of diffuse color.</p>\n<p>It must be an issue with the barycentric coordinates then. The barycentric coordinates\nI am generating have been well tested in the\n<a href=\"https://en.wikipedia.org/wiki/M%C3%B6ller%E2%80%93Trumbore_intersection_algorithm\">Moller-Trumbore intersection algorithm</a>\nI have been using for triangles. Therefore, it must be an issue with my interpolation.</p>\n<p>Indeed, permuting the barycentric coordinates in the correct fashion yields the correct image.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/77e55d6e5f6b38bd4fe1ca7476ffee03/d9199/render4.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.08108108108109%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACfklEQVQozz3O6U/aYACA8bflUloUhIIC5W5LpbRQziJHAQFhiuJQiayeDHUroDPToMl0WzbUJVvcFfdlyfaXLsuSPR9/nx4AAAAQBADwOjDpQDodXV6PXrVWV/+hfnIiHGTSKWGxVqVIAgAAKxTgfyoFpFbCMADuaf3e/ubV7fXd6HUhLTjsViUELEZD0E/GI3xaSBbEbKWYd9tnFAColbBKAYEpRGFElfpxiLCg/b0nP+4/fr55y5EOym0l3LjTbGAoMsaHU/FooVBdWNqlAnGDFsYQpRFRACUEKSEAA4Chqm6r8e3D6O7qIum3EpimOucJuO20zxsNsQIfjddYWsYJeYYMa0iDmjCrgWtKZdEpUTU8oQZSvfTl9t37i5clBst6NMsJE+vFaa+XZwOJEB8t+nxdmOzBQlqTdWpT3nFQJLVlP1LxIwVibKvAXR9tDbcbj8PmEjWW8OgJq5XxeUIMHWO5WJpybWtcHVUlo10L6lY4HVhkkAaLNkPoZkR3UKDPW9Vhq3IkunaT6KOAkbbbAz53aJaKMIwoUCuSriEhO/nJrqDfEyZBM4ysR9CNCNKO6w7L/vN2+bJdPi7R8tx0nbXSOM4Sbn6WjAYDlRh9tmY+axqPS6ZeziTnpoAUwZ5Gbb2E7yLF34jl7wvrP5ud34s7v/LSkCuLuJvCp2mPk6c8SzHP6ZrtpGkZzJsGoqkvYkBmmYfkxoOw8TXSuE8uf0rX7zK1N6nMMMkN5+i+YJNipnbcvJ+yDor207pzUJvp5Y1yzihnTaDpxbZsRIfDO1FHJ+M7rHD95aRcZbo568HfN+xYtJwU7C+KjkHR2S+5evO2Z1nz8wwmi5Y/CKGLQbYBqb8AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Correct Texture Render\"\n        title=\"Correct Texture Render\"\n        src=\"/static/77e55d6e5f6b38bd4fe1ca7476ffee03/fcda8/render4.png\"\n        srcset=\"/static/77e55d6e5f6b38bd4fe1ca7476ffee03/12f09/render4.png 148w,\n/static/77e55d6e5f6b38bd4fe1ca7476ffee03/e4a3f/render4.png 295w,\n/static/77e55d6e5f6b38bd4fe1ca7476ffee03/fcda8/render4.png 590w,\n/static/77e55d6e5f6b38bd4fe1ca7476ffee03/efc66/render4.png 885w,\n/static/77e55d6e5f6b38bd4fe1ca7476ffee03/d9199/render4.png 960w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>Now that textures are working properly, I can combine it with the depth of field effect to\nproduce a crisp render.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/b9859358a5ad58bd23d1b2a3299cc876/d9199/render5.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.08108108108109%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAAChElEQVQozy3O7VPScADA8d8YIAosEBjKw2gw9gADxhjoAGHyMETRU8CnIxAlhJKH1E5T6zozC8+687K6fNXVX9p51/fl59UXAAAABAEASBe629k5H737OLrcqz9TqlQAAMMTJBxgk3GxtFikKVIBw0rlo/9PBUNqpQKGgMdmaLdqVzeXX0ZXxew8TRJKCFhNxgBNxgQ+Kc7m5tNFOedx2WEIjKkUaiUEJrWwWac0jEOkVT/Yqz/cfb2/ueYZ3O91UW4XhhpZiozy4XgsksksLiy36EBiUquyaGGTFgZqGKhgoICAVa/ubJd/3H6+vXwb92Nei3opSfhxB0N4BC4ohoVYifMducmjp6QwQRrVXlQNcJNqGlHqxhRGDdRYke9H158uTuTAdMqtKYtokHAyHg8f8s+EeCFPkAOYeg2LKU0K0yY8EyBDamVaV2B0OXK8keHev2ic75QrEatMT4geo9duYwk3xzLRYCiaovHuON7TFNK69QCyyiGgxOrWgkiVQ7YjSCfDvNlaONsqdCV8d1ZfYs2Mw+EncM5H8axfEqlyE6k09c2MYV807s0aQCWs3+D1m4KuNoN0Zfq0Jl/UCq/yvl5iaiVoZzBn0IvzPlII+Asx32nVelI1H+Yt/bSllzaBesTSFuz9GHEWD4+k/M/i5u+N53+Xd/9kG2ecLDlxyjnFuF085V6Ouo/XbUfVqWHOPJQsAwkFByH2V3zrIbH9PVr5lli7k1ZvpdKHufRFnDtPMgPRUY+ZazNoK24bZh3HK67h0nR/3tRLm3opM9j0oE2MaoexloC157zdAjdYFXuLgf20rfP4hg4l62HGcZzDhllXP4/3c/aXKfRgztKTrP8Ad/iNVFKMX34AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Combined Render\"\n        title=\"Combined Render\"\n        src=\"/static/b9859358a5ad58bd23d1b2a3299cc876/fcda8/render5.png\"\n        srcset=\"/static/b9859358a5ad58bd23d1b2a3299cc876/12f09/render5.png 148w,\n/static/b9859358a5ad58bd23d1b2a3299cc876/e4a3f/render5.png 295w,\n/static/b9859358a5ad58bd23d1b2a3299cc876/fcda8/render5.png 590w,\n/static/b9859358a5ad58bd23d1b2a3299cc876/efc66/render5.png 885w,\n/static/b9859358a5ad58bd23d1b2a3299cc876/d9199/render5.png 960w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<h4>What’s Next?</h4>\n<p>The next steps for me are implementing the refractive BTDF described in the\nmicrofacet BSDF paper listed above. The glass cup is assumed to be made out of\nFlint glass, with an index of refraction of 1.51. In contrast, air has an index\nof refraction of 1.003.</p>\n<p>This BTDF isn’t fully finished yet. There are a lot of tweaks that need to be\nmade to the sampling procedure before it is ready, as well as possibly to\nthe evaluation and PDF functions. As you can see in the glass, the transmission\nis oddly mirrored, which is likely the result of an incorrect refraction\nimplementation. I am still actively looking into this.</p>\n<p>I’m hoping to have a basic form of volumetric path tracing implementing after\nthe glass BTDF is wrapped up, at which point my scene will be fully complete.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/84cb662b0de03034b3a7793d86648ed8/d9199/render6.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.08108108108109%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACfklEQVQozy3IaU/aYAAA4LcXKLSl2JZDNg6BChGQQxgItlChHGHKoYhDVGRMFuTwDposnhvOLTMuLvPTEn/psmTPxwcAAAAEAQA4k6bR3jobfboZXe7U6yiGAQBUKtIwqUdgiJmgHJwdQVAU/ff/YQgkQ2EEAtZJqtWsXd9dfhtdZxcTzmk7AMA368qlJRSBSRJfzufSSdFmMiAQkGOwDIXAhBJhcJQahzgd0dvZeH74/nh363dOuTiz2aBbXEhJUhFGIBWuKCxXMvmmwx2dUGKsEqGVCJAhAEMADAEtge2uF5/uv9xfnM/PGO2sPBu1pA55PhdUyCBKSSaqsZkDK3dg5gIKTi2za2TAQmN6EsPlsHoM2lySft7dfj4/ljx63jxW5DX8DyqZw1QK6BXL1lthex+dPoIj/BhvVEatCiBySsmBp514khvfFL0XHzeH26VyQCvZFG/8VPiKEGNyYhwOOUzHXcHUxi17soyAr7rJgpcEeRde9JArXvJdgNwVnafVzLAqtUVzw0culSjxBl8MYgwp52ct/Xqs3GRKW8ptUd2KqHfCFCj7iEqAWJvDayGyLTlOa9J5LT2QnJ2wvihoUwVVyo37DIQwpalE3cPq5HGZ3k+xXYHdE2hQD7CtOUM3ZBvO+0fx1K/M2p/V9y9vt1+SWwczop/SvyYIE0VyDBm3Mftl/aCs7SeZfpztxTWg43H9jlSfY+tPwZXHWOkhXviayF/F+LOIdyg6B0nDRoiuBunGguYwaztanurndN0E3RHoDs+AilXTME63fKbmnLG1YGunvf3CfDfn+SDodyWmt0QfJuhBQnciGU+y9r5k7SUNHUHT4dlOXPsXQU2MZKsq9L4AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Bad Glass Render\"\n        title=\"Bad Glass Render\"\n        src=\"/static/84cb662b0de03034b3a7793d86648ed8/fcda8/render6.png\"\n        srcset=\"/static/84cb662b0de03034b3a7793d86648ed8/12f09/render6.png 148w,\n/static/84cb662b0de03034b3a7793d86648ed8/e4a3f/render6.png 295w,\n/static/84cb662b0de03034b3a7793d86648ed8/fcda8/render6.png 590w,\n/static/84cb662b0de03034b3a7793d86648ed8/efc66/render6.png 885w,\n/static/84cb662b0de03034b3a7793d86648ed8/d9199/render6.png 960w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>\n<p>In meantime, here’s the scene rendered with a bronze cup instead of a glass cup!</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 590px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/724d2cc48437e9847717e93708966860/d9199/render7.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.08108108108109%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsTAAALEwEAmpwYAAACfklEQVQozz3B7VPScAAA4N/YxsvYfjJgGxOOF4EBBygCvgDCxttAMk5BUiIRMQ5MwSCNwq7TsqLzvPPy7Kovddc/2reeBwAAAIIAAAQb2+7uT6bvr6dX7b1nGI4DAGhCtcTrtn3804DZY4QorsIwDPyHo4gSU6AIcM7qOoeN669Xt9OP69m0V3BiCmCiNBEOFl2sbKWzIV+pILtssygCVLhCiSFAr0WNJKbTIAJHDdp7v+5u76efwj6H32UT5uw2ShlhKXmOyVpgNrRYKne8wYReizNa1KBFgRIFOAoUCOAovFvferj5cnP5Lu63uhnlozXXPAfDRm3WbsjyuliM84+cwsguRAiBVrpZJXAYcB7ipEpBq5FmWb6fXn++OJeDfGpOXVlllkxUxEhKVjpjomNhrTBAvWdoLKVOWbUJJwEyglb2kgUfmROIZmbh8qg52a9Uw5zsJeJ2KkwTMZYULTNZfibph46exnGiLopkLQg3QxBsBMitebgdgvUI7GZ8b3aLk91CT7K3YrDsgUk9mWTJtBnmeFhyw0qLqraoVkbXidHtVR3YXqRqEWonSjaWYU/2jhvyRaMwzPteJPi6V7/BUDkTJZvh+iysOeC4woxrzDDPDESmLxrAXoTpRM39ZdckHp5K+e/rO7+fPP/7+OBPtvkhkNpnjaJBk2aJIkc0XNRZhT+rcsOc8aXEDCQWHC8EfsR3fybqD8vVb4mtO2nzRtr4uCZeJEJvE57BCt+O6ttLxt4KM0qbzsu2YYk/TRv6oqEvGsGOkz2wejqL1sOotZN0HxUWBpur/fVgV+J7KX0/ZXglseOc5XXOPMpYTvOO05z5RGSPk8yJyP0DSIeIguJCSQ0AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <img\n        class=\"gatsby-resp-image-image\"\n        alt=\"Bronze Render\"\n        title=\"Bronze Render\"\n        src=\"/static/724d2cc48437e9847717e93708966860/fcda8/render7.png\"\n        srcset=\"/static/724d2cc48437e9847717e93708966860/12f09/render7.png 148w,\n/static/724d2cc48437e9847717e93708966860/e4a3f/render7.png 295w,\n/static/724d2cc48437e9847717e93708966860/fcda8/render7.png 590w,\n/static/724d2cc48437e9847717e93708966860/efc66/render7.png 885w,\n/static/724d2cc48437e9847717e93708966860/d9199/render7.png 960w\"\n        sizes=\"(max-width: 590px) 100vw, 590px\"\n        style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n        loading=\"lazy\"\n        decoding=\"async\"\n      />\n  </a>\n    </span></p>","frontmatter":{"title":"Realistic Path Tracing (part I)","date":"May 27, 2020","description":"Crafting a scene, implementing texture mapping and depth-of-field effects using OptiX"}}},"pageContext":{"slug":"/blog/2020-05-26-path-tracing-part-i/","previous":{"fields":{"slug":"/blog/2019-12-04-starting-a-blog/"},"frontmatter":{"title":"Starting My Own Blog"}},"next":{"fields":{"slug":"/blog/2020-06-07-path-tracing-part-ii/"},"frontmatter":{"title":"Realistic Path Tracing (part II)"}}}},"staticQueryHashes":["3000541721"]}